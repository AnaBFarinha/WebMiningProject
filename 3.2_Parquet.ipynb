{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Reviews Data - EDA\n",
    "\n",
    "The major objective of this notebook is to perform an Exploratory Data Analysis, also known as EDA, in the data related to games reviews. To do that, we start by joining our parquet files into a dataset. After, we ensure that each variable is in the correct format and start doing some simple explorations.\n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "\n",
    "[0. Import Libraries](#libraries) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Libraries<a id='libraries'></a>\n",
    "[to the top](#top)  \n",
    "\n",
    "The first step is to import the necessary libraries.\n",
    "These imports cover the necessary libraries for working with Parquet files, data manipulation, visualization, and text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concate Parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_parquet(directory_path):\n",
    "\n",
    "    # Get a list of Parquet files in the directory\n",
    "    parquet_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.parquet')]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through each Parquet file\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a DataFrame\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        \n",
    "        # Extract the substring before the underscore from the filename\n",
    "        filename = os.path.basename(file)\n",
    "        appid = filename.split('_')[0]\n",
    "        \n",
    "        # Add a new column with the extracted prefix value\n",
    "        df['AppId'] = appid\n",
    "        \n",
    "        # Append the modified DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not have_df:\n",
    "    parquets_df = concate_parquet('./data/parquets/')\n",
    "    parquets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column author contains a dictionary\n",
    "\n",
    "Transform the dictionary into columns of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_column(concatenated_df):\n",
    "    # Use json_normalize to expand the dictionary column into separate columns\n",
    "    expanded_df = pd.json_normalize(concatenated_df['author'])\n",
    "\n",
    "    # Concatenate the expanded DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([concatenated_df, expanded_df], axis=1)\n",
    "\n",
    "    # Drop the original dictionary column\n",
    "    result_df.drop('author', axis=1, inplace=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not have_df:\n",
    "    parquets_df = expand_column(parquets_df)\n",
    "    parquets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not have_df:\n",
    "    parquets_df['recommendationid'].duplicated().sum() # Cant be index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m     handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m )\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 324\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mwriters.pyx:73\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_df:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mparquets_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquets_df.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhandles\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Note: self.encoding is irrelevant here\u001b[39;49;00m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcsvlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:157\u001b[0m, in \u001b[0;36mIOHandles.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    153\u001b[0m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m     traceback: TracebackType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:144\u001b[0m, in \u001b[0;36mIOHandles.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles:\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "if not have_df:\n",
    "    parquets_df.to_csv('parquets_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if have_df:\n",
    "    parquets_df = pd.read_csv(\"parquets_df.csv\")\n",
    "    parquets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the basic information about the DataFrame\n",
    "To get a concise summary of the DataFrame.\n",
    "This helps in understanding the structure of the data, identifying any issues such as missing values or incorrect data types, and getting an overall sense of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the basic information about the DataFrame\n",
    "print(\"DataFrame Info:\")\n",
    "print(parquets_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing Values\n",
    "Understanding which columns have missing values helps in deciding how to handle them, such as filling them with appropriate values or dropping rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(parquets_df.isnull().sum()) # calculates the total number of missing values in each column of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates\n",
    "\n",
    "Duplicate rows can skew the analysis and should be addressed by either removing them or keeping them based on the context of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets_df.duplicated() # returns a boolean value (true for duplicates and false for no duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatype changes\n",
    "To ensure that each column has the correct data type for analysis.\n",
    "Converting data types can improve efficiency and accuracy of operations, and ensure consistency in data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datatypes(gamereviews):\n",
    "    # Handle missing values in the review column\n",
    "    gamereviews[\"review\"] = gamereviews[\"review\"].fillna(\"\")\n",
    "    \n",
    "    # Format \"timestamp_created\" to int\n",
    "    gamereviews[\"timestamp_created\"] = gamereviews[\"timestamp_created\"].astype('int64')\n",
    "    \n",
    "    # Format timestamps\n",
    "    gamereviews[\"timestamp_created\"] = pd.to_datetime(gamereviews[\"timestamp_created\"], unit='s', utc=True)\n",
    "    gamereviews[\"timestamp_updated\"] = pd.to_datetime(gamereviews[\"timestamp_updated\"], unit='s', utc=True)\n",
    "    gamereviews[\"last_played\"] = pd.to_datetime(gamereviews[\"last_played\"], unit='s', utc=True)\n",
    "    \n",
    "    # Remove unnecessary fields\n",
    "    gamereviews.drop(columns=[\"language\", \"hidden_in_steam_china\", \"steam_china_location\"], inplace=True)\n",
    "    \n",
    "    # Format boolean fields\n",
    "    gamereviews[\"voted_up\"] = gamereviews[\"voted_up\"].astype(bool)\n",
    "    gamereviews[\"steam_purchase\"] = gamereviews[\"steam_purchase\"].astype(bool)\n",
    "    gamereviews[\"received_for_free\"] = gamereviews[\"received_for_free\"].astype(bool)\n",
    "    gamereviews[\"written_during_early_access\"] = gamereviews[\"written_during_early_access\"].astype(bool)\n",
    "    gamereviews[\"weighted_vote_score\"] = gamereviews[\"weighted_vote_score\"].astype(float)\n",
    "    \n",
    "    return gamereviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets_df = format_datatypes(parquets_df)\n",
    "parquets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamp columns (timestamp_created, timestamp_updated, last_played) were converted to datetime objects using the pd.to_datetime() function to facilitate temporal analysis. Unnecessary fields such as language, hidden_in_steam_china, and steam_china_location were removed from the DataFrame to streamline the dataset. Boolean fields (voted_up, steam_purchase, received_for_free, written_during_early_access) were converted to boolean data types for clear representation. Additionally, the weighted_vote_score column was converted to a float data type to accurately reflect its numerical nature. These data type changes ensure that our DataFrame is ready for comprehensive analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Variables\n",
    "To isolate numerical variables from the DataFrame for further analysis.\n",
    "This step helps in focusing on numerical features that might be relevant for modeling or exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical variables\n",
    "# Remove Id variables\n",
    "numerical_df = parquets_df.select_dtypes(include=['int64', 'float64']).drop(['recommendationid', 'AppId', 'steamid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "Summary statistics provide insights into the central tendency, dispersion, and shape of the distribution of each numerical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to know the dataset\n",
    "numerical_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "To explore the linear relationship between pairs of numerical variables in the DataFrame.\n",
    "Helps in understanding how variables are related to each other, which can guide feature selection, dimensionality reduction, or predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between numerical columns\n",
    "correlation_matrix = numerical_df.corr()\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap produced visually represents the strength and direction of the linear relationship between pairs of numerical variables. Correlation coefficients close to 1 indicate a strong positive linear relationship, while coefficients close to -1 indicate a strong negative linear relationship. Coefficients close to 0 suggest little to no linear relationship.\n",
    "High correlations (either positive or negative) between variables may indicate redundancy or multicollinearity, which can affect the performance of predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Matrix of User's Data\n",
    "To visualize relationships between multiple numerical variables in the user's data.\n",
    "Scatter matrices provide a comprehensive view of pairwise relationships between variables, along with histograms of individual variables along the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter matrix\n",
    "scattermatrix = scatter_matrix(parquets_df[['num_games_owned', 'num_reviews', 'playtime_forever', 'playtime_last_two_weeks', 'playtime_at_review']],\n",
    "                                diagonal='hist', figsize=(15,10))\n",
    "\n",
    "# Get the number of columns in the matrix\n",
    "n = scattermatrix.shape[1]\n",
    "\n",
    "# Mask the upper triangle of the matrix\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        scattermatrix[i, j].set_visible(False)\n",
    "        \n",
    "# Rotate the axis labels\n",
    "for ax in scattermatrix.ravel():    \n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.xaxis.label.set_rotation(90)\n",
    "\n",
    "# Choose a title and make the title appear closer to the graph\n",
    "plt.suptitle(\"Scatter Matrix of Users Data\", y = 0.93)\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Matrix of Reviews' Data\n",
    "To visualize relationships between multiple numerical variables in the reviews' data.\n",
    "Similar to the previous scatter matrix, this visualization provides insights into pairwise relationships between variables and their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter matrix\n",
    "scattermatrix = scatter_matrix(parquets_df[['votes_up', 'votes_funny', 'weighted_vote_score', 'comment_count']],\n",
    "                                diagonal='hist', figsize=(15,10))\n",
    "\n",
    "# Get the number of columns in the matrix\n",
    "n = scattermatrix.shape[1]\n",
    "\n",
    "# Mask the upper triangle of the matrix\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        scattermatrix[i, j].set_visible(False)\n",
    "        \n",
    "# Rotate the axis labels\n",
    "for ax in scattermatrix.ravel():    \n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.xaxis.label.set_rotation(90)\n",
    "\n",
    "# Choose a title and make the title appear closer to the graph\n",
    "plt.suptitle(\"Scatter Matrix of Reviews' Data\", y = 0.93)\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Based Variables\n",
    "To visualize the distribution of the timestamp_created variable, which represents the time when reviews were created.\n",
    "Understanding the distribution of timestamps can provide insights into temporal patterns or trends in review creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of timestamp_created\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(data=parquets_df['timestamp_created'], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Timestamp Created\")\n",
    "plt.xlabel(\"Timestamp Created\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peaks in the distribution indicate periods of high review activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Distribution of Timestamp Created (2022-2024)\n",
    "To filter the data for timestamps between 2022 and 2024 and visualize the distribution of review creation timestamps during this period.\n",
    "Additionally, to analyze the distribution of game contributions (reviews) for each game during the same period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for timestamps between 2022 and 2024 - Change dates using the peaks of the data\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-12-31'\n",
    "filtered_df = parquets_df[(parquets_df['timestamp_created'] >= start_date) & (parquets_df['timestamp_created'] <= end_date)]\n",
    "\n",
    "# Plot the distribution of timestamp_created between 2022 and 2024\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(data=filtered_df['timestamp_created'], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Timestamp Created (2022-2024)\")\n",
    "plt.xlabel(\"Timestamp Created\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of game contributions in the same period\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(data=filtered_df, y='AppId', order=filtered_df['AppId'].value_counts().index)\n",
    "plt.title(\"Game Contributions (2022-2024)\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"AppId\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot visualizes the frequency distribution of review creation timestamps between 2022 and 2024. This helps in identifying any temporal patterns or trends in review activity during this period.\n",
    "The second plot displays the distribution of game contributions (reviews) for each game during the same period, allowing for the identification of popular games or those with the highest number of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis - Monthly Count of Voted Up Reviews\n",
    "To perform time series analysis on the count of \"voted up\" reviews over monthly intervals, this helps in understanding patterns, trends, and seasonality in the data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "parquets_df.set_index('timestamp_created', inplace=True)\n",
    "plt.figure(figsize=(12, 8))\n",
    "parquets_df['voted_up'].resample('M').count().plot()\n",
    "plt.title(\"Monthly Count of Voted Up Reviews\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Analysis - Filtered \"EDIT:\" Reviews\n",
    "To filter reviews that start with \"EDIT:\" and analyze their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter reviews that start with \"EDIT:\"\n",
    "edit_reviews = parquets_df[parquets_df['review'].str.startswith('EDIT:')]\n",
    "print(len(edit_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of reviews that start with \"EDIT:\" provides insights into the frequency of edited reviews, which may indicate user interactions or updates to previous reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length Histogram\n",
    "To visualize the distribution of review lengths using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Review Lengths\n",
    "parquets_df['review_length'] = parquets_df['review'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(parquets_df['review_length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Review Lengths')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of review lengths visualizes the distribution of review lengths, allowing for the identification of common or uncommon review lengths and their frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Games with the Longest Preprocessed Review Lengths\n",
    "To identify and visualize the top 10 games with the longest preprocessed review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_longest = parquets_df.sort_values(by='review_length', ascending=True).tail(10)\n",
    "\n",
    "# Plotting the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_longest['AppId'], top_10_longest['review_length'], color='skyblue') # Change AppID to Games name\n",
    "plt.xlabel('ID')\n",
    "plt.ylabel('Length of Preprocessed Review')\n",
    "plt.title('Length of Preprocessed Review by ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame is sorted based on the review length in descending order to identify the games with the longest preprocessed review lengths. A horizontal bar plot is generated using Matplotlib to visualize the review lengths of these top 10 games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Response Time in Days\n",
    "To calculate the response time in days between the timestamp created and the timestamp when the developer responded. To check for any NaN values in the response time and drop rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to datetime objects and make them timezone-naive\n",
    "parquets_df['timestamp_dev_responded'] = pd.to_datetime(parquets_df['timestamp_dev_responded']).dt.tz_localize(None)\n",
    "parquets_df['timestamp_created'] = pd.to_datetime(parquets_df['timestamp_created']).dt.tz_localize(None)\n",
    "\n",
    "# Calculate response time in days\n",
    "parquets_df['response_time'] = (parquets_df['timestamp_created'] - parquets_df['timestamp_dev_responded']).dt.days\n",
    "\n",
    "# Check for any NaN values in response_time\n",
    "print(parquets_df['response_time'].isna().sum(), \"NaN values in response_time\")\n",
    "\n",
    "# Drop rows with NaN response_time if any\n",
    "parquets_df = parquets_df.dropna(subset=['response_time'])\n",
    "\n",
    "# Distribution of Response Time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(parquets_df['response_time'], bins=30, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Response Time in Days')\n",
    "plt.xlabel('Response Time (days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram displays the distribution of response time in days, indicating how quickly developers responded to user reviews.\n",
    "Analyzing the distribution can provide insights into the typical response time and any outliers or patterns in developer responsiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using VADER\n",
    "To calculate the sentiment polarity of each review using VADER, visualize the distribution of review sentiment polarity and explore its relationship with the number of votes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to calculate sentiment polarity using VADER\n",
    "def get_vader_sentiment(review):\n",
    "    return sid.polarity_scores(review)['compound']\n",
    "\n",
    "# Calculate sentiment polarity for each review\n",
    "parquets_df['sentiment'] = parquets_df['review'].apply(get_vader_sentiment)\n",
    "\n",
    "# Distribution of Sentiment\n",
    "plt.figure(figsize=(18, 6))  # Changed the width to 18 for a wider plot\n",
    "sns.histplot(parquets_df['sentiment'], bins=30, kde=True, color='purple')\n",
    "plt.title('Distribution of Review Sentiment')\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment vs. Votes Up\n",
    "plt.figure(figsize=(18, 6))  # Changed the width to 18 for a wider plot\n",
    "sns.scatterplot(x='sentiment', y='votes_up', data=parquets_df, alpha=0.5)\n",
    "plt.title('Sentiment vs. Votes Up')\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Votes Up')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram provides insights into the distribution of sentiment polarity across all reviews, indicating the prevalence of positive, negative, or neutral sentiments.\n",
    "The scatter plot explores the relationship between sentiment polarity and the number of votes up, allowing for the examination of any patterns or correlations between review sentiment and user engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Relationship Between Response Time and Votes Up\n",
    "To explore the relationship between response time (in days) and the number of votes up received by reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate response time in days\n",
    "parquets_df['response_time'] = (pd.to_datetime(parquets_df['timestamp_dev_responded']) - pd.to_datetime(parquets_df['timestamp_created'])).dt.days\n",
    "\n",
    "# Response Time vs. Votes Up\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='response_time', y='votes_up', data=parquets_df, alpha=0.5)\n",
    "plt.title('Response Time vs. Votes Up')\n",
    "plt.xlabel('Response Time (days)')\n",
    "plt.ylabel('Votes Up')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot allows for the examination of any correlations or patterns between response time and user engagement, as indicated by the number of votes up received by reviews.\n",
    "By analyzing the distribution of points, we can identify whether there is a relationship between response time and user engagement. For example, you may observe that reviews with shorter response times tend to receive more votes up, indicating higher user satisfaction or engagement.\n",
    "Understanding this relationship can help developers or platform administrators prioritize timely responses to reviews to enhance user satisfaction and engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Function\n",
    "To preprocess text data by removing noise and irrelevant information, making it suitable for natural language processing tasks such as sentiment analysis or text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''Takes a string as input and returns a preprocessed string as output'''\n",
    "    \n",
    "    #remove punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    #lowercase\n",
    "    text = [word.lower() for word in text]\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    \n",
    "    #remove numbers\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "\n",
    "    # Join\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    #remove isolated consonants:\n",
    "    text = re.sub(r'\\b([^aeiou-])\\b',' ',text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Visualization of Preprocessed Text Data\n",
    "To visually explore the most frequent words in the preprocessed text data, providing insights into the common themes or topics present in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocess_text function to each sentence in the DataFrame\n",
    "parquets_df['review_preprocessed'] = parquets_df['review'].apply(preprocess_text)\n",
    "\n",
    "# Join the preprocessed sentences into a single string\n",
    "wordcloud_input = ' '.join(parquets_df['review_preprocessed'])\n",
    "\n",
    "# Create a wordcloud\n",
    "wordcloud = WordCloud(width = 800, height = 600, \n",
    "                background_color ='white', \n",
    "                max_font_size = 50, \n",
    "                colormap='Dark2').generate(wordcloud_input)\n",
    "\n",
    "# Plot the WordCloud image\n",
    "plt.figure(figsize = (8, 6), facecolor = None)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word cloud allows for a quick understanding of the predominant themes or topics present in the reviews.\n",
    "Words that appear larger and more prominently in the word cloud are those that occur frequently across multiple reviews.\n",
    "Analyzing the word cloud can reveal insights into the sentiments, preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie Chart: Voted Up vs. Voted Down Reviews\n",
    "To visualize the distribution of reviews based on whether they were voted up or down by users, providing an overview of user sentiment towards the reviewed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart of Voted Up vs. Voted Down Reviews\n",
    "voted_counts = parquets_df['voted_up'].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(voted_counts, labels=voted_counts.index, autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "plt.title('Voted Up vs. Voted Down Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A larger portion of reviews in the \"Voted Up\" category suggests that the majority of users have positive sentiments towards the reviewed content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
