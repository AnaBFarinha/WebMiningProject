{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4d4b11",
   "metadata": {},
   "source": [
    "# Evaluation <a id='top'></a>\n",
    "\n",
    "In this last notebook, we start evaluating our newly trained recommendation systems for new users. \n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "\n",
    "[0. Import Libraries](#libraries) <br>\n",
    "[1. Create Necessary Functions](#functions) <br>\n",
    "&emsp; [1.1. Evaluation Function](#evaluate) <br>\n",
    "&emsp; [1.2. Plot Evaluation Function](#plot_evaluate) <br>\n",
    "&emsp; [1.3. Evaluate and Visualize Function](#evaluate_visualize) <br>\n",
    "[2. Evaluate Recommendation Systems ](#apply_functions) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aefbf2",
   "metadata": {},
   "source": [
    "# 0. Import libraries <a id='libraries'></a>\n",
    "[to the top](#top)\n",
    "\n",
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38356c-9382-46ee-8fcf-b03c911c4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "from model import UserBasedCF, ItemBasedCF, MatrixFactorizationCF\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from helper_functions import load_config, load_data, format_timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587549c",
   "metadata": {},
   "source": [
    "# 1. Create Necessary Functions <a id='functions'></a>\n",
    "[to the top](#top)\n",
    "\n",
    "In this section of the notebook, we start by defining the functions we are going to use to evaluate the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724ad7b",
   "metadata": {},
   "source": [
    "## 1.1. Evaluation Function <a id='evaluate'></a>\n",
    "[to the top](#top)\n",
    "\n",
    "This function evaluates the performance of a recommendation model by comparing its predictions against the actual ratings in the test dataset. It calculates several metrics including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Precision, Recall, and F1-score. Precision, Recall, and F1-score are computed using a binary classification approach, assuming a threshold of 3 for relevant ratings. These metrics provide insights into the accuracy and effectiveness of the recommendation model in predicting user preferences and can help guide further model refinement and optimization efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c478c8-5452-4aac-bb63-edecbd031c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for row in test_data.itertuples():\n",
    "        user_id = row.user_id\n",
    "        game_id = row.game_id\n",
    "        actual_rating = row.rating\n",
    "\n",
    "        prediction = model.predict(user_id, game_id)\n",
    "        y_true.append(actual_rating)\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Root Mean Squared Error\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Precision, Recall, F1-score (assuming threshold for relevant rating is > 3)\n",
    "    y_true_binary = (y_true > 3).astype(int)\n",
    "    y_pred_binary = (y_pred > 3).astype(int)\n",
    "\n",
    "    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'Precision': precision, 'Recall': recall, 'F1-score': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfacd0",
   "metadata": {},
   "source": [
    "## 1.2. Plot Evaluation Function <a id='plot_evaluate'></a>\n",
    "[to the top](#top)\n",
    "\n",
    "This function plots the evaluation results of a recommendation model, visualizing various performance metrics. It takes a dictionary of results containing metric names as keys and corresponding scores as values, along with the name of the model being evaluated. The function extracts metric labels and scores from the dictionary, plots them as a bar chart, and labels the axes appropriately. The resulting plot provides a concise overview of the model's performance across different evaluation metrics, aiding in the comparison and interpretation of its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66670161-2a6a-430a-9e12-7d9874f9a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(results, model_name):\n",
    "    labels, values = zip(*results.items())\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, values, color='b')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'{model_name} Evaluation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2f211",
   "metadata": {},
   "source": [
    "## 1.3. Evaluate and Visualize Function <a id='evaluate_visualize'></a>\n",
    "[to the top](#top)\n",
    "\n",
    "This function simplifies the evaluation and visualization process for the recommendation sytems. It loads test data, pre-trained models, and evaluates each model's performance using the evaluate_model function. The evaluation results are then printed, providing insights into the models' performance. Additionally, the function plots the evaluation results for each model, presenting a visual comparison of their performance across various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774bea1-3289-48d9-b591-42f78fb6d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_and_show_results(reviews_dir, game_details_path, selection_file, selection_name=\"\"):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Starting evaluation for selection: {selection_name}\")\n",
    "    _, test_data = load_data(reviews_dir, game_details_path, selection_file)\n",
    "    data_loading_time = time.time()\n",
    "    print(f\"Data loaded in {format_timedelta(timedelta(seconds=data_loading_time - start_time))}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Evaluate User-Based CF model\n",
    "    print(f\"Loading User-Based CF model for selection: {selection_name}\")\n",
    "    user_based_model = UserBasedCF.load(f'data/model/{selection_name}_user_based_cf_model.pkl')\n",
    "    user_based_loading_time = time.time()\n",
    "    print(f\"User-Based CF model loaded in {format_timedelta(timedelta(seconds=user_based_loading_time - data_loading_time))}\")\n",
    "    \n",
    "    print(f\"Evaluating User-Based CF model for selection: {selection_name}\")\n",
    "    user_based_results = evaluate_model(user_based_model, test_data)\n",
    "    user_based_evaluation_time = time.time()\n",
    "    print(f\"User-Based CF model evaluated in {format_timedelta(timedelta(seconds=user_based_evaluation_time - user_based_loading_time))}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Free memory\n",
    "    del user_based_model\n",
    "    \n",
    "    # Evaluate Item-Based CF model\n",
    "    print(f\"Loading Item-Based CF model for selection: {selection_name}\")\n",
    "    item_based_model = ItemBasedCF.load(f'data/model/{selection_name}_item_based_cf_model.pkl')\n",
    "    item_based_loading_time = time.time()\n",
    "    print(f\"Item-Based CF model loaded in {format_timedelta(timedelta(seconds=item_based_loading_time - user_based_evaluation_time))}\")\n",
    "    \n",
    "    print(f\"Evaluating Item-Based CF model for selection: {selection_name}\")\n",
    "    item_based_results = evaluate_model(item_based_model, test_data)\n",
    "    item_based_evaluation_time = time.time()\n",
    "    print(f\"Item-Based CF model evaluated in {format_timedelta(timedelta(seconds=item_based_evaluation_time - item_based_loading_time))}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Free memory\n",
    "    del item_based_model\n",
    "    \n",
    "    # Evaluate Matrix Factorization CF model\n",
    "    print(f\"Loading Matrix Factorization CF model for selection: {selection_name}\")\n",
    "    matrix_factorization_model = MatrixFactorizationCF.load(f'data/model/{selection_name}_matrix_factorization_cf_model.pkl')\n",
    "    matrix_factorization_loading_time = time.time()\n",
    "    print(f\"Matrix Factorization CF model loaded in {format_timedelta(timedelta(seconds=matrix_factorization_loading_time - item_based_evaluation_time))}\")\n",
    "    \n",
    "    print(f\"Evaluating Matrix Factorization CF model for selection: {selection_name}\")\n",
    "    matrix_factorization_results = evaluate_model(matrix_factorization_model, test_data)\n",
    "    matrix_factorization_evaluation_time = time.time()\n",
    "    print(f\"Matrix Factorization CF model evaluated in {format_timedelta(timedelta(seconds=matrix_factorization_evaluation_time - matrix_factorization_loading_time))}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Free memory\n",
    "    del matrix_factorization_model\n",
    "    \n",
    "    print(f\"Results for selection: {selection_name}\")\n",
    "    print(\"User-Based CF Evaluation Results:\", user_based_results)\n",
    "    print(\"Item-Based CF Evaluation Results:\", item_based_results)\n",
    "    print(\"Matrix Factorization CF Evaluation Results:\", matrix_factorization_results)\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_evaluation_results(user_based_results, f'User-Based CF {selection_name}')\n",
    "    plot_evaluation_results(item_based_results, f'Item-Based CF {selection_name}')\n",
    "    plot_evaluation_results(matrix_factorization_results, f'Matrix Factorization CF {selection_name}')\n",
    "    \n",
    "    plotting_time = time.time()\n",
    "    print(f\"Results plotted in {format_timedelta(timedelta(seconds=plotting_time - matrix_factorization_evaluation_time))}\")\n",
    "    \n",
    "    total_time = plotting_time - start_time\n",
    "    print(f\"Total evaluation time for selection {selection_name}: {format_timedelta(timedelta(seconds=total_time))}\")\n",
    "    print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3f220",
   "metadata": {},
   "source": [
    "# 2. Evaluate Recommendation Systems <a id='apply_functions'></a>\n",
    "[to the top](#top)\n",
    "\n",
    "Finally, we apply our new functions to evaluate our recommendation systems with different metrics and plot the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f408a2-1155-4fc2-8ffb-8b2cbc13680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "for variation, params in config.items():\n",
    "    get_evaluate_and_show_results(\n",
    "        params['reviews_dir'],\n",
    "        params['game_details_path'],\n",
    "        params['selection_file'],\n",
    "        params['selection_name']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
