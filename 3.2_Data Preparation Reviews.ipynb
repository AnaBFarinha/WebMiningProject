{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d56ea3b",
   "metadata": {},
   "source": [
    "# Data Preparation - Reviews <a id='top'></a>\n",
    "\n",
    "In this last notebook of data preparation, the data that we will be using is the one about data reviews. Here, we will make some transformations to the reviews feature by cleaning a bit the text, exclude really small reviews, unpacks several variables from unother column, removes unecessary columns and formats boolean features.\n",
    "\n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "\n",
    "[0. Import Libraries](#libraries) <br>\n",
    "[1. Define Functions](#functions) <br>\n",
    "&emsp; [1.1. Clean Reviews](#clean) <br>\n",
    "&emsp; [1.2. Prepapre Data](#prepare) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd90f6",
   "metadata": {},
   "source": [
    "# 0. Import Libraries<a id='libraries'></a>\n",
    "[to the top](#top)  \n",
    "\n",
    "The first step is to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "from helper_functions import clean_parquet_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85a94d",
   "metadata": {},
   "source": [
    "# 1. Define Functions<a id='functions'></a>\n",
    "[to the top](#top) \n",
    "\n",
    "TO facilitate the process of data preparation, we start by defining the functions that we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd280161",
   "metadata": {},
   "source": [
    "## 1.1. Clean Reviews<a id='clean'></a>\n",
    "[to the top](#top)  \n",
    "\n",
    "The function below is designed to clean text reviews by removing unwanted characters and formatting. It first checks if the input is a string, raising an error if not. The function then removes all non-ASCII characters using a regular expression, ensuring the text contains only standard English characters. Additionally, it eliminates newline and carriage return characters. These steps help standardize reviews for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b952eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    # Remove non-ASCII characters\n",
    "    review = re.sub(r'[^\\x00-\\x7F]', '', review)\n",
    "    # Remove newline and carriage return characters\n",
    "    review = review.replace('\\n', '').replace('\\r', '')\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf928d1f",
   "metadata": {},
   "source": [
    "## 1.2. Prepare Data<a id='prepare'></a>\n",
    "[to the top](#top) \n",
    "\n",
    "The preprocess_gamereviews function is designed to clean and standardize game review data stored in dictionaries. It begins by preprocessing the review text to remove unwanted characters and ignoring reviews shorter than 20 characters. It then formats timestamp fields to UTC datetime objects for consistency. The function also unpacks nested fields from the author section into flat fields for easier access and removes unnecessary fields to streamline the data. Additionally, it ensures boolean fields are correctly typed and converts certain fields to floats. The cleaned and standardized dictionary is then returned, ready for further analysis or storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gamereviews(gamereviews_dict):\n",
    "    \n",
    "    # Preprocess the review\n",
    "    gamereviews_dict[\"review\"] = preprocess_review(gamereviews_dict[\"review\"])\n",
    "    \n",
    "    # Skip rows with reviews less than 20 characters\n",
    "    if len(gamereviews_dict[\"review\"]) < 20:\n",
    "        return None\n",
    "    \n",
    "    # Format timestamps\n",
    "    gamereviews_dict[\"timestamp_created\"] = datetime.fromtimestamp(gamereviews_dict[\"timestamp_created\"], tz=timezone.utc)\n",
    "    gamereviews_dict[\"timestamp_updated\"] = datetime.fromtimestamp(gamereviews_dict[\"timestamp_updated\"], tz=timezone.utc)\n",
    "    gamereviews_dict[\"author\"][\"last_played\"] = datetime.fromtimestamp(gamereviews_dict[\"author\"][\"last_played\"], tz=timezone.utc)\n",
    "    \n",
    "    # Unpack nested author fields\n",
    "    gamereviews_dict[\"user_steamid\"] = gamereviews_dict[\"author\"][\"steamid\"]\n",
    "    gamereviews_dict[\"user_num_games_owned\"] = gamereviews_dict[\"author\"][\"num_games_owned\"]\n",
    "    gamereviews_dict[\"user_num_reviews\"] = gamereviews_dict[\"author\"][\"num_reviews\"]\n",
    "    gamereviews_dict[\"user_playtime_forever\"] = gamereviews_dict[\"author\"][\"playtime_forever\"]\n",
    "    gamereviews_dict[\"user_playtime_at_review\"] = gamereviews_dict[\"author\"][\"playtime_at_review\"]\n",
    "    gamereviews_dict[\"user_last_played\"] = gamereviews_dict[\"author\"][\"last_played\"]\n",
    "    \n",
    "    # Remove unnecessary fields\n",
    "    gamereviews_dict.pop(\"author\")\n",
    "    gamereviews_dict.pop(\"language\", None)\n",
    "    gamereviews_dict.pop(\"hidden_in_steam_china\", None)\n",
    "    gamereviews_dict.pop(\"steam_china_location\", None)\n",
    "    \n",
    "    # Format boolean and float fields\n",
    "    gamereviews_dict[\"voted_up\"] = bool(gamereviews_dict[\"voted_up\"])\n",
    "    gamereviews_dict[\"steam_purchase\"] = bool(gamereviews_dict[\"steam_purchase\"])\n",
    "    gamereviews_dict[\"received_for_free\"] = bool(gamereviews_dict[\"received_for_free\"])\n",
    "    gamereviews_dict[\"written_during_early_access\"] = bool(gamereviews_dict[\"written_during_early_access\"])\n",
    "    gamereviews_dict[\"weighted_vote_score\"] = float(gamereviews_dict[\"weighted_vote_score\"])\n",
    "    \n",
    "    # Convert back to a DataFrame row\n",
    "    return gamereviews_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc152bf",
   "metadata": {},
   "source": [
    "## 1.2. Prepare Data<a id='prepare'></a>\n",
    "[to the top](#top) \n",
    "\n",
    "The preprocess_parquet_file function processes game review data stored in a Parquet file and prepares it for further analysis. It begins by reading the Parquet file into a DataFrame and applying the preprocess_gamereviews function to each row to clean and standardize the review data. Rows that do not meet the criteria, such as reviews shorter than 20 characters, are filtered out. The cleaned data is then compiled into a new DataFrame, with an additional column for the appid of the game. The function also ensures there are no duplicate rows in the DataFrame. Finally, the processed DataFrame is written back to a Parquet file at the specified output path, ready for efficient storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parquet_file(filepath, output_filepath, appid):\n",
    "    # Read the parquet file\n",
    "    df = pl.read_parquet(filepath)\n",
    "    \n",
    "    # Apply preprocessing to each row\n",
    "    preprocessed_data = [preprocess_gamereviews(row) for row in df.iter_rows(named=True) if row is not None]\n",
    "    \n",
    "    # Filter out None values (reviews less than 20 characters)\n",
    "    preprocessed_data = [row for row in preprocessed_data if row is not None]\n",
    "    \n",
    "    # Create a new DataFrame from preprocessed data\n",
    "    if preprocessed_data:\n",
    "        preprocessed_df = pl.DataFrame(preprocessed_data)\n",
    "        \n",
    "        # Add the appid column\n",
    "        preprocessed_df = preprocessed_df.with_column(pl.lit(appid).alias(\"appid\"))\n",
    "        \n",
    "        # Check for duplicates\n",
    "        preprocessed_df = preprocessed_df.unique()\n",
    "        \n",
    "        # Write the processed file\n",
    "        preprocessed_df.write_parquet(output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867fd2e",
   "metadata": {},
   "source": [
    "# 2. Data Preparation<a id='preparation'></a>\n",
    "[to the top](#top) \n",
    "\n",
    "Before applying our newly created functions, we have to define in which parquet files we are going to use them. TO do that, we created another function clean_parquet_files  that is designed to manage and clean Parquet files in a specified directory by focusing on the count of reviews for each app. It uses a regular expression to extract the app ID and review count from each filename and stores the highest count file for each app in a dictionary. Initially, it scans all Parquet files, retaining only those with more than 250 reviews and identifying the file with the highest count for each app. In the second step, it iterates through the files again, deleting any that either have a count of 250 or fewer reviews or are not the file with the highest count for their respective app ID. This ensures that only the most substantial and relevant Parquet file for each app is kept, optimizing storage and data management.\n",
    "\n",
    "Before applying this new function, we define where the parquet files input and output folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'data/parquets'\n",
    "output_folder = 'data/parquets_preprocessed'\n",
    "\n",
    "clean_parquet_files(input_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132d47e",
   "metadata": {},
   "source": [
    "Below, we apply our functions to clean the data in the selected parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd45372e-c4be-43bd-b20b-e2b9c4a87cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "from helper_functions import clean_parquet_files\n",
    "\n",
    "def preprocess_review(review):\n",
    "    # Remove non-European characters\n",
    "    review = re.sub(r'[^\\x00-\\x7F]', '', review)\n",
    "    # Remove newline and carriage return characters\n",
    "    review = review.replace('\\n', '').replace('\\r', '')\n",
    "    return review\n",
    "\n",
    "def preprocess_gamereviews(gamereviews_dict):\n",
    "    # Preprocess the review\n",
    "    gamereviews_dict[\"review\"] = preprocess_review(gamereviews_dict.get(\"review\", \"\"))\n",
    "    \n",
    "    # Skip rows with reviews less than 20 characters\n",
    "    if len(gamereviews_dict[\"review\"]) < 20:\n",
    "        return None\n",
    "    \n",
    "    # Format timestamps\n",
    "    gamereviews_dict[\"timestamp_created\"] = datetime.fromtimestamp(gamereviews_dict.get(\"timestamp_created\", 0), tz=timezone.utc)\n",
    "    gamereviews_dict[\"timestamp_updated\"] = datetime.fromtimestamp(gamereviews_dict.get(\"timestamp_updated\", 0), tz=timezone.utc)\n",
    "    \n",
    "    # Ensure author field is not None\n",
    "    author = gamereviews_dict.get(\"author\", {})\n",
    "    \n",
    "    # Unpack nested author fields\n",
    "    gamereviews_dict[\"user_steamid\"] = author.get(\"steamid\", \"\")\n",
    "    gamereviews_dict[\"user_num_games_owned\"] = author.get(\"num_games_owned\", 0)\n",
    "    gamereviews_dict[\"user_num_reviews\"] = author.get(\"num_reviews\", 0)\n",
    "    gamereviews_dict[\"user_playtime_forever\"] = author.get(\"playtime_forever\", 0)\n",
    "    \n",
    "    # Remove unnecessary fields\n",
    "    gamereviews_dict.pop(\"author\", None)\n",
    "    gamereviews_dict.pop(\"author_last_played\", None)\n",
    "    gamereviews_dict.pop(\"language\", None)\n",
    "    gamereviews_dict.pop(\"hidden_in_steam_china\", None)\n",
    "    gamereviews_dict.pop(\"steam_china_location\", None)\n",
    "    \n",
    "    # Format boolean and float fields\n",
    "    gamereviews_dict[\"voted_up\"] = bool(gamereviews_dict.get(\"voted_up\", False))\n",
    "    gamereviews_dict[\"steam_purchase\"] = bool(gamereviews_dict.get(\"steam_purchase\", False))\n",
    "    gamereviews_dict[\"received_for_free\"] = bool(gamereviews_dict.get(\"received_for_free\", False))\n",
    "    gamereviews_dict[\"written_during_early_access\"] = bool(gamereviews_dict.get(\"written_during_early_access\", False))\n",
    "    gamereviews_dict[\"weighted_vote_score\"] = float(gamereviews_dict.get(\"weighted_vote_score\", 0.0))\n",
    "    \n",
    "    # Convert back to a DataFrame row\n",
    "    return gamereviews_dict\n",
    "\n",
    "def preprocess_parquet_file(filepath, output_filepath, appid):\n",
    "    # Read the parquet file\n",
    "    df = pl.read_parquet(filepath)\n",
    "    \n",
    "    # Apply preprocessing to each row\n",
    "    preprocessed_data = [preprocess_gamereviews(row) for row in df.iter_rows(named=True) if row is not None]\n",
    "    \n",
    "    # Filter out None values (reviews less than 20 characters)\n",
    "    preprocessed_data = [row for row in preprocessed_data if row is not None]\n",
    "    \n",
    "    # Create a new DataFrame from preprocessed data\n",
    "    if preprocessed_data:\n",
    "        preprocessed_df = pl.DataFrame(preprocessed_data)\n",
    "        \n",
    "        # Add the appid column\n",
    "        preprocessed_df = preprocessed_df.with_columns(pl.lit(appid).alias('appid'))\n",
    "        \n",
    "        # Check for duplicates\n",
    "        preprocessed_df = preprocessed_df.unique()\n",
    "        \n",
    "        # Write the processed file\n",
    "        preprocessed_df.write_parquet(output_filepath)\n",
    "\n",
    "input_folder = 'data/parquets'\n",
    "output_folder = 'data/parquets_preprocessed'\n",
    "\n",
    "clean_parquet_files(input_folder)\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    match = re.match(r\"(\\d+)_reviews_\\d+\\.parquet\", file_name)\n",
    "    if match:\n",
    "        appid = match.group(1)\n",
    "        input_filepath = os.path.join(input_folder, file_name)\n",
    "        output_filepath = os.path.join(output_folder, file_name.replace('.parquet', '_preprocessed.parquet'))\n",
    "        \n",
    "        # Check if the processed file already exists\n",
    "        if not os.path.exists(output_filepath):\n",
    "            preprocess_parquet_file(input_filepath, output_filepath, appid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
